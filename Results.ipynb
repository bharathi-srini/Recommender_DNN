{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, concatenate, Embedding, Reshape\n",
    "from keras.layers import Merge, Flatten, merge, Lambda, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras import backend as K\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'product_id', 'up_orders', 'up_first_order', 'up_last_order',\n",
       "       'up_average_cart_position', 'prod_orders', 'prod_reorder_probability',\n",
       "       'prod_reorder_ratio', 'user_orders', 'user_period',\n",
       "       'user_mean_days_since_prior', 'user_total_products',\n",
       "       'user_distinct_products', 'user_average_basket', 'order_id',\n",
       "       'days_since_prior_order', 'up_order_rate', 'up_orders_since_last_order',\n",
       "       'aisle_id', 'department_id', 'order_dow', 'order_hour_of_day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfolder = 'F:/rs/Recommender_DNN/input/'\n",
    "df = pd.read_csv(myfolder + 'data_subset.csv')\n",
    "y = df['reordered'].values\n",
    "del df['reordered']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df['prod_reorder_probability']\n",
    "del df['prod_reorder_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"order_dow\", \"order_hour_of_day\"]\n",
    "CONTINUOUS_COLUMNS = [ \"user_orders\", \"days_since_prior_order\",\"up_orders\",\"up_first_order\",\"up_last_order\",\"up_average_cart_position\",\"prod_orders\",\"user_period\",\"user_distinct_products\",\"user_mean_days_since_prior\",\"user_total_products\", \"user_average_basket\",\"up_order_rate\",\"up_orders_since_last_order\"]\n",
    "EMBEDDING_COLUMNS = [\"user_id\", \"product_id\",\"aisle_id\",\"department_id\"]\n",
    "#\"prod_reorder_ratio\",\"prod_reorder_probability\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One-hot encoding categorical columns\n",
    "df = pd.get_dummies(df, columns=[x for x in CATEGORICAL_COLUMNS])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalising the feature columns\n",
    "df[CONTINUOUS_COLUMNS] = pd.DataFrame(MinMaxScaler().fit_transform(df[CONTINUOUS_COLUMNS]), columns=CONTINUOUS_COLUMNS)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper to index columns before embeddings\n",
    "def val2idx(df, cols):\n",
    "    val_types = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique()\n",
    "\n",
    "    val_to_idx = dict()\n",
    "    for k, v in val_types.items():\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "\n",
    "    for k, v in val_to_idx.items():\n",
    "        df[k] = df[k].apply(lambda x: v[x])\n",
    "\n",
    "    unique_vals = dict()\n",
    "    for c in cols:\n",
    "        unique_vals[c] = df[c].nunique()\n",
    "\n",
    "    return df, unique_vals\n",
    "#Using Keras layer to create Embeddings\n",
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, embeddings_regularizer=l2(reg))(inp)\n",
    "#Input layers for continuous vectors to the deep network\n",
    "def continous_input(name):\n",
    "    inp = Input(shape=(1,), dtype='float32', name=name)\n",
    "    return inp, Reshape((1, 1))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting datasets into train and test\n",
    "df.reset_index()\n",
    "gc.collect()\n",
    "X = df[df.columns.difference(['user_id','product_id','order_id'])].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining input column for the deep network\n",
    "DEEP_COLNS = EMBEDDING_COLUMNS + CONTINUOUS_COLUMNS\n",
    "df_deep, unique_vals = val2idx(df, EMBEDDING_COLUMNS)\n",
    "X_deep_tr, X_deep_te, y_deep_tr, y_deep_te = train_test_split(df_deep, y, test_size=0.25, random_state=42)\n",
    "#Creating input dataframe for the merged model\n",
    "X_train_deep = [X_deep_tr[c] for c in DEEP_COLNS]\n",
    "y_train_deep = np.array(y_deep_tr).reshape(-1, 1)\n",
    "X_test_deep = [X_deep_te[c] for c in DEEP_COLNS]\n",
    "y_test_deep = np.array(y_deep_te).reshape(-1, 1)\n",
    "gc.collect()\n",
    "#Inputs\n",
    "X_tr_wd = [X_train] + X_train_deep\n",
    "Y_tr_wd = y_train_deep  # wide or deep is the same here\n",
    "X_te_wd = [X_test] + X_test_deep\n",
    "Y_te_wd = y_test_deep  # wide or deep is the same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py:1269: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#Loading pre-trained models\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('wide.json', 'r')\n",
    "wide_json = json_file.read()\n",
    "json_file.close()\n",
    "wide = model_from_json(wide_json)\n",
    "# load weights into new model\n",
    "wide.load_weights(\"wide.h5\")\n",
    "json_file = open('deep.json', 'r')\n",
    "deep_json = json_file.read()\n",
    "json_file.close()\n",
    "deep = model_from_json(deep_json)\n",
    "# load weights into new model\n",
    "deep.load_weights(\"deep.h5\")\n",
    "json_file = open('wide_deep.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "wide_deep = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "wide_deep.load_weights(\"wide_deep.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wide.compile(Adam(0.1), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "deep.compile(Adam(0.1), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "wide_deep.compile(optimizer=Adam(lr=0.1), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031154/1031154 [==============================] - 50s 48us/step\n",
      "Wide: [0.42105097553864146, 0.8075040197682589]\n",
      "1031154/1031154 [==============================] - 140s 135us/step\n",
      "Deep: [1.114770540285856, 0.7945893629856638]\n",
      "1031154/1031154 [==============================] - 144s 139us/step\n",
      "Wide and Deep: [1.0514775766263171, 0.8024233043757173]\n"
     ]
    }
   ],
   "source": [
    "results_w = wide.evaluate(X_test, y_test)\n",
    "print(\"Wide:\",results_w)\n",
    "results_d = deep.evaluate(X_test_deep, y_test_deep)\n",
    "print(\"Deep:\",results_d)\n",
    "results_wd = wide_deep.evaluate(X_te_wd, Y_te_wd)\n",
    "print(\"Wide and Deep:\",results_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_metrics(y_test, prd):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, prd)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"auc: \",auc)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test, prd)    \n",
    "    auprc  = metrics.auc(recall, precision)\n",
    "    max_f1 = 0\n",
    "    for r, p, t in zip(recall, precision, thresholds):\n",
    "        if p + r == 0: continue\n",
    "        if (2*p*r)/(p + r) > max_f1:\n",
    "            max_f1 = (2*p*r)/(p + r) \n",
    "            max_f1_threshold = t\n",
    "    print(\"max_f1: \",max_f1)\n",
    "    print(\"max_f1_threshold: \",max_f1_threshold)\n",
    "    log_loss = metrics.log_loss(y_test, prd)\n",
    "    print(\"log_loss: \",log_loss)\n",
    "    return auc, max_f1, max_f1_threshold, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prd_w = wide.predict(X_test)\n",
    "prd_d = deep.predict(X_test_deep)\n",
    "prd_wd = wide_deep.predict(X_te_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:  0.7986973935526261\n",
      "max_f1:  0.5222984720904563\n",
      "max_f1_threshold:  0.17564811\n",
      "log_loss:  0.4210509770792291\n",
      "auc:  0.5\n",
      "max_f1:  0.3408143759594003\n",
      "max_f1_threshold:  0.21945308\n",
      "log_loss:  0.5083970414376625\n",
      "auc:  0.8155017773438071\n",
      "max_f1:  0.5444768983101497\n",
      "max_f1_threshold:  0.42255753\n",
      "log_loss:  0.43932537743489186\n"
     ]
    }
   ],
   "source": [
    "auc_w, max_f1_w, max_f1_threshold_w, log_loss_w = get_metrics(y_test, prd_w)\n",
    "auc_d, max_f1_d, max_f1_threshold_d, log_loss_d = get_metrics(y_test, prd_d)\n",
    "auc_wd, max_f1_wd, max_f1_threshold_wd, log_loss_wd = get_metrics(y_test, prd_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
